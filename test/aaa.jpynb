import warnings

warnings.filterwarnings("ignore")
import subprocess
import sys
import pandas as pd
import numpy as np
import pickle
import hashlib

# def install(package):
#     subprocess.check_call([sys.executable,"-m","pip","install", package])
    
# install('folium')
# install('seaborn')
# install('xgboost')

from aicentro.session import Session4
sacp_session = Session (verify = False)
from aicentro.framework.keras import Keras as SacpFrm
sacp_framework = SacpFrm(session = sacp_session)
1. 데이터 수집
ansan_data = pd.read_csv(sacp_framework.config.data_dir + '/ansan_data.csv')

ansan_data.head()
# 암호화방법
# 암호화 함수 정의
def encrypt(target):
    hashSHA = hashlib.sha256()
    hashSHA.update(target.encode('utf-8'))
    return hashSHA.hexdigest().upper()
    
encrypt("홍길동")

#IS 값 암호화
ansan_data['IS'] = str(ansan_data['IS'])
ansan_data['IS'] = ansan_data['IS'].apply(encrypt)
ansan_data['JOIN_SEQ'] = str(ansan_data['JOIN_SEQ'])
ansan_data['JOIN_SEQ'] = ansan_data['JOIN_SEQ'].apply(encrypt)
ansan_data.head()

# 주소에 대한 위경도 좌표 수집
with open(sacp_framework.config.data_dir+'/json_data.pickle', 'rb') as f:
    json_data = pickle.load(f) #단 한줄씩 읽어옴
    
json_data[:10]

json_data[0].json()['documents'][0]['address_name']

# 좌표 가져오기
json_data[0].json()['documents'][0]['x'],json_data[0].json()['documents'][0]['y']

address_data=pd.DataFrame([])
# 에러 발생
for i in np.arange(len(json_data)):
    address_data = address_data.append([(json_data[i].json()['documents'][0]['address_name'],
                                        json_data[i].json()['documents'][0]['x'],
                                       json_data[i].json()['documents'][0]['y'])],
                                      ignore_index = True)

json_data[18].json()
address_data=pd.DataFrame([])

for i in np.arange(len(json_data)):
    if json_data[i].json()['documents'] != []:
        address_data = address_data.append([(json_data[i].json()['documents'][0]['address_name'],
                                            json_data[i].json()['documents'][0]['x'],
                                           json_data[i].json()['documents'][0]['y'])],
                                          ignore_index = True)
                                          
address_data

# 데이터 전처리

ansan_data.info()
# 결측값 제거
ansan_data.drop(['JOIN_DATE', 'DATE', 'C_PERIOD','B_DONG_ADD', 'ROAD_NM_ADD', 'CNT'], axis= 1, inplace = True)

ansan_data.BUILDING_NM.fillna("", inplace = True)
ansan_data[ansan_data['BUILDING_NM'].isna()]['BUILDING_NM']

ansan_data[ansan_data['BUILDING_NM'].isna()]

#lon, lat(경도, 위도) 의 결측값이 있는 행 삭제
ansan_data.dropna(axis = 0, inplace = True)
ansan_data.info()


# 중복제거
ansan_data = ansan_data.drop_duplicates()
ansan_data.info()

#컬럼명 변경

ansan_data = ansan_data.rename({'상세분류2': 'TYPE_DTL_NM'}, axis = 1)

ansan_data

# Feature Engineering Domain 지식 - 데이터 삭제

ansan_data = ansan_data.loc[(ansan_data['PRODUCT']!= '059Z')]


# Feature Engineering Domain 지식 - 데이터 수정
pd.DataFrame(ansan_data.groupby(['TYPE','TYPE_DTL_NM'])['PRODUCT'].count())


ansan_data.loc[(ansan_data['TYPE']=='공공기관') |
               (ansan_data['TYPE']=='기타') |
               (ansan_data['TYPE']=='법인사업자') |
               (ansan_data['TYPE']=='정식단체'),'TYPE'] = '법인'
ansan_data.loc[(ansan_data['TYPE']=='개인') 
               &((ansan_data['TYPE_DTL_NM']=='AM') |
               (ansan_data['TYPE_DTL_NM']=='CORE') |
               (ansan_data['TYPE_DTL_NM']=='가치') |
               (ansan_data['TYPE_DTL_NM']=='법인')),'TYPE'] = '개인사업자'

ansan_data.loc[ansan_data['TYPE_DTL_NM']=='ㆍ값없음']

ansan_data.loc[(ansan_data['TYPE']=='개인') 
               &(ansan_data['TYPE_DTL_NM']=='ㆍ값없음'),'TYPE'] =='개인 사업자' 

ansan_data.groupby("TYPE")['IS'].count()

# 데이터 저장
ansan_data.to_csv(sacp_framework.config.data_dir + '/ansan_data_F.csv', encoding = 'utf-8', header =True, index = False)

# 데이터 시각화 ###
from matplotlib import pyplot as plt
import seaborn as sns

import matplotlib.font_manager as fm
fm.get_fontconfig_fonts()
plt.rc('font', family = 'NanumGothicCoding')
plt.rcParams['figure.figsize'] = (12,9)

# baeplot
prod_stat = pd.DataFrame(ansan_data.groupby(['TYPE', 'PRODUCT_NM'])['PRODUCT'].count()).sort_values(by=['TYPE', 'PRODUCT'], ascending = False).reset_index()
prod_stat
prod_stat = prod_stat.rename(columns={'PRODUCT':'CNT'})
prod_stat

i = 1
for c_type in ['개인','개인사업자', '법인']:
    plt.subplot(3,1,i)
    graph_data = prod_stat[prod_stat['TYPE']==c_type]
    sns.barplot(data = graph_data, x = 'PRODUCT_NM', y = 'CNT')
    i = i+1
    
plt.show()

#상품 통합

def etctoetc(df, threshold):
    total_cnt = df['CNT'].sum()
    cnt = 0
    for n in df.index:
        cnt = cnt+df.loc[n,'CNT']
        if(cnt > total_cnt*threshold):
            df.loc[n:,'PRODUCT_NM'] = '기타상품'
            break
    df = pd.DataFrame(df.groupby(['TYPE','PRODUCT_NM'])['CNT'].sum()).reset_index()
    
    df2 = df.loc[df['PRODUCT_NM']=='기타상품',:]
    df3 = df.loc[df['PRODUCT_NM']=='기타상품',:].sort_values(by = ['CNT'], ascending = False)
    df4 = pd.concat([df3,df2], sort = False).reset_index(drop = True)
    
    return df4

i = 1

for c_type in ['개인', '개인사업자', '법인']:
    plt.subplot(3,1,i)
    graph_data = etctoetc(prod_stat[prod_stat['TYPE'] ==c_type],0.9)
    sns.barplot(data = graph_data, x='PRODUCT_NM', y='CNT')
    i = i+1
    3
plt.show()

# Folium 을 이용한 지도 시각화
import folium
ansan_data.describe()

f_lon = 126.813232
f_lat = 37.3178579

map = folium.Map(location = [f_lat,f_lon], zoom_start = 10)
map


map_ST = folium.Map(location = [f_lat,f_lon], zoom_start = 14, tiles = 'Stamen Terrain')
map_ST

#지도위에 데이터 올리기
from folium.plugins import HeatMap
ansan_map_1 = ansan_data.loc[ansan_data['TYPE'] == '개인',:]
ansan_map_2 = ansan_data.loc[ansan_data['TYPE'] == '개인사업자',:]
ansan_map_3 = ansan_data.loc[ansan_data['TYPE'] == '법인',:]

heat_data = np.array([ansan_map_1['lat'], ansan_map_1['lon']])
heat_data = heat_data.transpose()

map = folium.Map(location = [f_lat,f_lon], zoom_start = 13 )
HeatMap(heat_data, min_opacity = 0.2, max_val =1 , max_zoom = 25, radius = 25).add_to(map)
map


heat_data = np.array([ansan_map_2['lat'], ansan_map_2['lon']])
heat_data = heat_data.transpose()

map = folium.Map(location = [f_lat,f_lon], zoom_start = 13 )
HeatMap(heat_data, min_opacity = 0.2, max_val =1 , max_zoom = 25, radius = 25).add_to(map)
map


heat_data = np.array([ansan_map_3['lat'], ansan_map_3['lon']])
heat_data = heat_data.transpose()

map = folium.Map(location = [f_lat,f_lon], zoom_start = 13 )
HeatMap(heat_data, min_opacity = 0.2, max_val =1 , max_zoom = 25, radius = 25).add_to(map)
map

#모델 train/test

ansan_data_T = ansan_data.loc[:,['TYPE','PRODUCT_NM','ROAD_NM','BUILDING_NO1','BUILDING_NO2']]
ansan_data_T.head()
ansan_data

N = int(len(ansan_data_T) * 0.9)
train_data = ansan_data_T.sample(n=N)
len(train_data)

test_data = ansan_data_T[140650:]
train_data.to_csv(sacp_framework.config.data_dir + '/ansan_train.csv',
                 header = True, index = False, encoding = 'utf-8')
test_data.to_csv(sacp_framework.config.data_dir + '/ansan_test.csv',
                 header = True, index = False, encoding = 'utf-8')

train_data = pd.read_csv(sacp_framework.config.data_dir + '/ansan_train.csv')
test_data = pd.read_csv(sacp_framework.config.data_dir + '/ansan_test.csv')

#label 인코딩

le_columns = train_data.columns
le_columns

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()

for column in le_columns:
    le.fit(train_data[column])
    train_data[column] = le.transform(train_data[column])
    
    for label in np.unique(test_data[column]):
        if label not in le.classes_:
            le.classes_ = np.append(le.classes_, label)
    test_data[column] = le.transform(test_data[column])

train_data.shape, test_data.shape

train_data_le = train_data
test_data_le = test_data

train_data_le.to_csv(sacp_framework.config.data_dir+ '/ansan_train_le.csv',
                    header = True, index = False, encoding = 'utf-8')
test_data_le.to_csv(sacp_framework.config.data_dir+ '/ansan_test_le.csv',
                    header = True, index = False, encoding = 'utf-8')
train_data_le = pd.read_csv(sacp_framework.config.data_dir + '/ansan_train_le.csv')
test_data_le = pd.read_csv(sacp_framework.config.data_dir + '/ansan_test_le.csv')

train_data_le.shape

train_data_le

from sklearn.model_selection import train_test_split

x_train_le, x_val_le, y_train_le, y_val_le = train_test_split(train_data_le.drop('TYPE',1),train_data_le['TYPE'], test_size = 0.2, random_state = 21, stratify = train_data_le['TYPE'])

x_train_le.shape, x_val_le.shape, y_train_le.shape, y_val_le.shape

x_test_le = test_data_le.iloc[:,1:]
y_test_le = test_data_le.iloc[:,0]

# 모델별 성능을 저장하기 위한 함수 선언
# 함수 외울 필요 없이 저장해두고 copy해서 사용
import matplotlib.pyplot as plt
import seaborn as sns

my_predictions = {}

colors = ['r', 'c','m','y', 'k', 'khaki', 'teal', 'orchid', 'sandybrown',
          'greenyellow','dodgerblue', 'deepskyblue', 'rosybrown', 'firebrick',
         'deeppink', 'crimson', 'salmon', 'darkred', 'olivedrab', 'olive',
         'forestgreen', 'royalblue', 'indigo', 'navy', 'mediumpurple', 'chocolate',
         'gold','darkorange','seagreen','turquoise','steelblue','slategray',
          'peru','midinightblue','slateblue','dimgray','cadetblue','tomato']
          
def acc_eval(name_, pred, actual):
    global predictions
    global colors
    
    acc = (pred == actual).mean()
    my_predictions[name_] = acc
    
    y_value = sorted(my_predictions.items(), key = lambda x : x[1], reverse = False)
    
    df = pd.DataFrame(y_value, columns=['model','acc'])
    
    min_ = df['acc'].min() - 0.5
    max_ = 1.2
    
    length = len(df)
    
    plt.figure(figsize = (10,length))
    ax = plt.subplot()
    ax.set_yticks(np.arange(len(df)))
    ax.set_yticklabels(df['model'], fontsize =15)
    bars = ax.barh(np.arange(len(df)), df['acc'])
    
    for i,v in enumerate(df['acc']):
        idx = np.random.choice(len(colors))
        bars[i].set_color(colors[idx])
        ax.text(v+0.1,i, str(round(v,3)), color = 'k', fontsize = 15, fontweight='bold')
        
    plt.title('Accuracy',fontsize = 18)
    plt.xlim(min_,max_)
    
    plt.show()
    
def remove_model(name_):
    global my_predictions
    try:
        del my_predicrions[name_]
    except KeyError:
        return False
    return True

from sklearn.tree import DecisionTreeClassifier
model_dt = DecisionTreeClassifier(min_samples_split = 2,
                                 min_samples_leaf = 1,
                                 max_features = None, 
                                 max_depth=None,
                                 max_leaf_nodes= None)

model_dt.fit(x_train_le, y_train_le)
pred_dt= model_dt.predict(x_test_le)
acc_eval('Decision Tree', pred_dt, y_test_le)

# Random Forest

from sklearn.ensemble import RandomForestClassifier

model_rf = RandomForestClassifier(n_jobs = -1, n_estimators= 300,
                                min_samples_split = 2,
                                 min_samples_leaf = 1,
                                 max_features = 'auto', 
                                 max_depth=None,
                                 max_leaf_nodes= None)

model_rf.fit(x_train_le, y_train_le)
pred_rf= model_rf.predict(x_test_le)
acc_eval('Random Forest', pred_rf, y_test_le)

# Xgboost

install("xgboost")

import xgboost as xgb
model_xgb = xgb.XGBClassifier()


dtrain = xgb.DMatrix(data = x_train_le, label = y_train_le)
dval = xgb.DMatrix(data = x_val_le, label = y_val_le)
dtest = xgb.DMatrix(data = x_test_le, label = y_test_le)

params = {'eta' : 0.3,
         'max_depth' :6,
         'min_child_weight':1,
         'gamma':0,
         'subsample':1,
         'colsample_bytree': 1,
         'objective':'multi:softmax',
         'eval_metric': 'mlogloss',
         'num_class':3 }


import time
start = time.time()

wlist = [(dtrain,'train'), (dval,'eval')]

model_xgb = xgb.train(params = params, dtrain = dtrain, num_boost_round = 300, evals = wlist, early_stopping_rounds = 20,)

(time.time()-start)/60

pred_xgb = model_xgb.predict(dtest)

acc_eval('XGBoost', pred_xgb, y_test_le)


# 평가지표 활용 Confusion matrix
y_true = np.array(y_test_le)
y_pred = pred_xgb
y_label = ['개인','개인사업자','법인']


sacp_framework.plot_confusion_matrix(
    y_true = y_true,
    y_pred = y_pred,
    target_names = y_label.
    title = 'Confusion Matrix',
    cmap = None,
    normalize = False
)

from sklearn.metrics import precosopm_score, recall_score
from sklearn.metrics impore f1_score, accuracy_score

def metric_func(actual, pred):
    print("accuracy_score:", accuracy_score(actual,pred))
    print("precision_score:",precision_score(actual,pred,average= None))
    print("recall_score:", recall_score(actual,pred, average= None))
    print("f1_score:",f1_score(actual,pred, average=None))

metric_func(y_test_le,pred_xgb)


# ROC Curve & AUC

sacp_framework.plot_roc_curve(
    y_true = pd.get_dummies(y_true).to_numpy(),
    y_pred = pd.get_dummies(y_pred).to_numpy(),

    n_classes = len(y_label),
    target_names = y_label,
    title = 'ROC Curve'
)

pred_xgb_margin = model_xgb.predict(dtest, output_margin = True)

sacp_framework.plot_roc_curve(
    y_true = pd.get_dummies(y_true).to_numpy(),
    y_pred =pred_xgb_margin,

    n_classes = len(y_label),
    target_names = y_label,
    title = 'ROC Curve'
)



